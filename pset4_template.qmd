---
title: "Your Title"
format: 
  pdf:
    keep-tex: true
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
---

**PS4:** Due Sat Nov 2 at 5:00PM Central. Worth 100 points. 

## Style Points (10 pts)
## Submission Steps (10 pts)
## Download and explore the Provider of Services (POS) file (10 pts)

1. What are the Variables I pulled? 
```{python}
import pandas as pd 
import altair as alt
import os
import geopandas as gpd

```

```{python}
# Reading dataset
path = "/Users/tsaili-ting/Uchicago/Year2/Y2Fall/Python2/problem-set-4-gena-ting/"

path_2016 = os.path.join(path, "pos2016.csv")
pos2016 = pd.read_csv(path_2016)

#variable pull 

pos2016.columns
```

1. Provider Category
2. Provider Category sub
3. City
4. Hospital Name
5. CMS Number
6. Termination code
7. Termination or Expiration Date
8. ZIP code

2. 
    a. How many hospitals are reported in this data? 
```{python}
pos2016_s = pos2016.loc[(pos2016["PRVDR_CTGRY_CD"]==1)&(pos2016["PRVDR_CTGRY_SBTYP_CD"]==1)]

len(pos2016_s)
```

    The number of short-term hospital in the dataset is 7245.
    
    b.Does this number make sense? Cross-reference with other sources and cite the numberyou compared it to.

     However, according to American Hospital Association(https://www.aha.org/statistics/fast-facts-us-hospitals), there are only 5129 community hospital. Citing that "Excluded are hospitals not accessible by the general public, such as prison hospitals or college infirmaries." in AHA's number. 
    
3. Plot the number of observations in your dataset by year.
```{python}
# Read 2017,18,19 dataset 
path_2017 = os.path.join(path, "pos2017.csv")
path_2018 = os.path.join(path, "pos2018.csv")
path_2019 = os.path.join(path, "pos2019.csv")

pos2017 = pd.read_csv(path_2017)
pos2018 = pd.read_csv(path_2018,encoding="latin1")
pos2019 = pd.read_csv(path_2019,encoding="latin1")

```

```{python}
# filter only short term hospital 
pos2017_s = pos2017.loc[(pos2017["PRVDR_CTGRY_CD"]==1)&(pos2017["PRVDR_CTGRY_SBTYP_CD"]==1)]
pos2018_s = pos2018.loc[(pos2018["PRVDR_CTGRY_CD"]==1)&(pos2018["PRVDR_CTGRY_SBTYP_CD"]==1)]
pos2019_s = pos2019.loc[(pos2019["PRVDR_CTGRY_CD"]==1)&(pos2019["PRVDR_CTGRY_SBTYP_CD"]==1)]

```

```{python}
# Add a column represent the fisical year, and append data

pos2016_s["Year"] = "2016"
pos2017_s["Year"] = "2017"
pos2018_s["Year"] = "2018"
pos2019_s["Year"] = "2019"

# Append the dataset
pos_combine = pd.concat([pos2016_s, pos2017_s, pos2018_s, pos2019_s], ignore_index=True)
```

```{python}
# plot the observations by year
alt.data_transformers.disable_max_rows()
alt.Chart(pos_combine).mark_bar().encode(
    x=alt.X('Year:N',  
            title="Year"),
    y=alt.Y('count()', title="Number of Observation",scale=alt.Scale(domain=[7000, 7500]))
).properties( title='Number of Observations over Year')
```

4. 
    a. Plot the number of unique hospitals in your dataset per year.

```{python}
# aggregate by year and calculate the unique number 
unique_hospital = pos_combine.groupby('Year')['PRVDR_NUM'].nunique().reset_index()
```
```{python}

alt.Chart(unique_hospital).mark_bar().encode(
    x=alt.X('Year:N', title="Year"),
    y=alt.Y('PRVDR_NUM:Q', title="Number of Unique Observations")
    ).properties(
    title='Number of Observations over Year'
)
```

b. Compare this to your plot in the previous step. What does this tell you about the structure of the data?

Look at the two graph they are identical, but we can also check the exact number of each year. 

```{python}
pos_combine.groupby("Year").agg(NumberOfHospital = ("Year","count")).reset_index()
```

```{python}
unique_hospital
```

They are exactly the same number, meaning each row is a unique hospital. 

## Identify hospital closures in POS file (15 pts) (*)

1. 
2. 
3. 
    a.
    b.
    c.

## Download Census zip code shapefile (10 pt) 

1. 
    a.
    - gz_2010_us_860_00_500k.dbf: It's an Attribute Table. This file contains attribute data for each shape (e.g., names, population, or other data related to each spatial feature). It’s essentially a database in a table format and is essential for linking spatial features to their descriptive information.    

    - gz_2010_us_860_00_500k.shp: It's a Shapefile. It contains the geometry data or spatial information. such as the points, lines, or polygons that make up the shape of each feature in the dataset. 

    - gz_2010_us_860_00_500k.xml: It's a Metadata File. This file includes metadata, such as a description of the data, its source, creation date, and other details about the dataset. 

    - gz_2010_us_860_00_500k.prj: It's a Projection File. This file holds information about the coordinate system and projection. It ensures that the shapefile aligns properly on a map with other spatial data.      

    - gz_2010_us_860_00_500k.shx: It's a Shape Index File. This index file is used to facilitate quick access to the geometries in the .shp file. It provides the spatial index of the geometry, helping software read specific features quickly without searching through the entire .shp file.

    b.  
    - gz_2010_us_860_00_500k.dbf: 6.4MB      
    - gz_2010_us_860_00_500k.shp: 837.5MB      
    - gz_2010_us_860_00_500k.xml: 16KM
    - gz_2010_us_860_00_500k.prj: 165bytes      
    - gz_2010_us_860_00_500k.shx: 265KB
2. 

```{python}
# read the shapefile 
filepath = "/Users/tsaili-ting/Uchicago/Year2/Y2Fall/Python2/problem-set-4-gena-ting/gz_2010_us_860_00_500k"
data = gpd.read_file(filepath)
type(data)
```

```{python}
# select Texas zip code, import pandas as pd
data['ZCTA5'] = data['ZCTA5'].astype(int)

# Filter Texas ZIP code range (73301 - 88595)
texas_shp = data[((data['ZCTA5'] >= 75000) & (data['ZCTA5'] <= 80000)) | (data['ZCTA5'] == 73301)]
```

```{python}
# calculate number of hospital under each zip code in 2016
pos2016_count = pos2016_s.groupby("ZIP_CD").agg(count = ("FAC_NAME","count")).reset_index()

# filter Texas
pos2016_count_tx = pos2016_count[(pos2016_count['ZIP_CD'] >= 73301) & (pos2016_count['ZIP_CD'] <= 88595)].reset_index()

```

```{python}
# merge shape and hospital by zip code

texas_merged = texas_shp.merge(pos2016_count_tx, left_on='ZCTA5', right_on='ZIP_CD', how='left')

texas_merged['count'] = texas_merged['count'].fillna(0)
texas_merged = texas_merged.drop(columns=['ZIP_CD', 'index'])
```

```{python}
# create choropleth of the number of hospitals by zip code in Texas
texas_merged.plot(column="count", cmap = "Blues",legend=True).set_axis_off()
```

## Calculate zip code’s distance to the nearest hospital (20 pts) (*)

1. 
2. 
3. 
4. 
    a.
    b.
    c.
5. 
    a.
    b.
    c.
    
## Effects of closures on access in Texas (15 pts)

1.

```{python}
# filter data to Texas
texas_combine = pos_combine[((pos_combine["ZIP_CD"] >= 75001) & (pos_combine["ZIP_CD"] <= 80000)) | (pos_combine["ZIP_CD"] == 73301)]

# find out closure in each year 
texas_closure = texas_combine.groupby(["ZIP_CD", "Year"]).agg(
     number_of_closure=("TRMNTN_EXPRTN_DT", lambda x: (x > 20160000).sum())
     ).reset_index()

# make zip a nuique row 
texas_closure = texas_closure.pivot_table(index = "ZIP_CD",columns = "Year", values = "number_of_closure").reset_index().fillna(0)

```

```{python}
# filter the number of closure > 0
texas_closure = texas_closure[
    (texas_closure["2016"] != 0) | 
    (texas_closure["2017"] != 0) | 
    (texas_closure["2018"] != 0) | 
    (texas_closure["2019"] != 0)].reset_index()

texas_closure

```

2. 

```{python}
# merge shape and number of closure hospital by zip code

texas_closure_merged = texas_shp.merge(texas_closure, left_on='ZCTA5', right_on='ZIP_CD', how='left')
texas_closure_merged["2019"] = texas_closure_merged["2019"].fillna(0)
```

```{python}
# create choropleth of the number of hospitals by zip code in Texas
texas_closure_merged.plot(column="2019", cmap = "Blues",legend=True).set_axis_off()

```

```{python}
len(texas_closure)
```

There are 27 zip code in Texas that at least has one closure during 2016-2019. 


3. (Partner 1) Then identify all the indirectly affected zip codes: Texas zip codes within a
10-mile radius of the directly affected zip codes. To do so, first create a GeoDataFrame
of the directly affected zip codes. Then create a 10-mile buffer around them. Then, do
a spatial join with the overall Texas zip code shapefile. How many indirectly affected
zip codes are there in Texas?


```{python}
# filter direct zip code
texas_direct = texas_closure_merged[texas_closure_merged["2019"] > 0].copy()
texas_direct.plot().set_axis_off()
```

```{python}
# create buffer
texas_buffer = texas_direct.copy()
texas_buffer['geometry'] = texas_buffer.geometry.buffer(0.145)
texas_buffer.plot(color="red", alpha=0.5).set_axis_off()
```

```{python}
# spatial join back to texas_closure 
texas_indirect = gpd.sjoin(texas_shp, texas_buffer, how="inner", predicate="intersects")
texas_indirect.plot(color="blue", alpha=0.3).set_axis_off()
```

```{python}
len(texas_indirect)
```

There are 795 zip code in Texas that is within 10 miles buffer, has indirect effect from hospital closure

4.  (Partner 1) Make a choropleth plot of the Texas zip codes with a different color for
each of the 3 categories: directly affected by a closure, within 10 miles of closure but
not directly affected, or not affected.

```{python}
# find out unaffect zip code
affected_zip_codes = set(texas_direct['ZIP_CD']).union(set(texas_indirect['ZIP_CD']))
texas_unaffected = texas_shp[~texas_shp['ZCTA5'].isin(affected_zip_codes)]
texas_unaffected.plot(color="green", alpha=0.3).set_axis_off()

```
```{python}
import matplotlib.pyplot as plt

fig, ax = plt.subplots()
texas_unaffected.plot(ax=ax, color="green", alpha=0.3)
texas_indirect.plot(ax=ax, color="blue", alpha=0.3)
texas_direct.plot(ax=ax, color="red", alpha=0.5)

ax.set_axis_off()
plt.show()

ax.set_axis_off()
plt.show()
```


## Reflecting on the exercise (10 pts) 
1.
	Currently, we identify real closures by comparing the number of active hospitals. However, this approach has a potential issue: we might mistakenly identify a genuine closure as a merger if the total number remains unchanged. This could lead to misclassifying a true closure as a merger.

	To improve this method, we could add a classification for operational closures. Instead of simply labeling them as “terminated,” an additional column indicating operational closure status would make it easier to verify the true nature of each hospital’s closure.